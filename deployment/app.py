# app.py
"""
ä¸€ä¸ªç”¨äºå›¾åƒå­—å¹•ç”Ÿæˆçš„Gradio Webåº”ç”¨ã€‚
"""
import gradio as gr
import torch
from torchvision import transforms
from PIL import Image
import argparse

# å¯¼å…¥æˆ‘ä»¬é¡¹ç›®ä¸­çš„è¾…åŠ©æ¨¡å—
import model_builder
import utils
import vocabulary


# --- 1. è®¾ç½® ---
device = "cuda" if torch.cuda.is_available() else "cpu"

# å®šä¹‰ä¸è®­ç»ƒæ—¶ç›¸åŒçš„å›¾åƒå˜æ¢
transform = transforms.Compose([
    transforms.Resize((299, 299)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# --- 2. åŠ è½½è¯æ±‡è¡¨å’Œæ¨¡å‹ ---
VOCAB_PATH = "." # å½“å‰ç›®å½•
MODEL_PATH = "captioning_model_best.pth" # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦æ›´æ”¹è¿™ä¸ªæ–‡ä»¶å

print("[INFO] æ­£åœ¨åŠ è½½è¯æ±‡è¡¨å’Œæ¨¡å‹...")
vocab = utils.load_vocab(VOCAB_PATH)
vocab_size = len(vocab)

# å®ä¾‹åŒ–ä¸€ä¸ªä¸è®­ç»ƒæ—¶é…ç½®ç›¸åŒçš„æ¨¡å‹
model = model_builder.EncoderDecoder(
    embed_size=512,
    hidden_size=512,
    vocab_size=vocab_size,
    num_layers=2,
    num_heads=8
).to(device)

# åŠ è½½æ¨¡å‹æƒé‡
try:
    model.load_state_dict(torch.load(MODEL_PATH, map_location=device))
    model.eval()
    print("[SUCCESS] æ¨¡å‹åŠ è½½æˆåŠŸï¼")
except FileNotFoundError:
    print(f"[ERROR] æ¨¡å‹æ–‡ä»¶æœªæ‰¾åˆ°: {MODEL_PATH}")
    gr.Error(f"æ¨¡å‹æ–‡ä»¶ '{MODEL_PATH}' æœªæ‰¾åˆ°ï¼è¯·æ£€æŸ¥éƒ¨ç½²æ–‡ä»¶å¤¹ã€‚")
    exit()
except Exception as e:
    print(f"[ERROR] åŠ è½½æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    gr.Error(f"åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
    exit()

# --- 3. åˆ›å»ºæ ¸å¿ƒé¢„æµ‹å‡½æ•° ---
def predict(image: Image.Image) -> str:
    """
    æ¥æ”¶ä¸€ä¸ªPILå›¾åƒï¼Œè¿”å›ç”Ÿæˆçš„å­—å¹•å­—ç¬¦ä¸²ã€‚
    """
    if image is None:
        return "é”™è¯¯ï¼šæœªæä¾›å›¾ç‰‡ã€‚"

    image_tensor = transform(image).to(device)

    model.eval()
    result_caption_indices = [vocab.stoi["<SOS>"]]
    
    with torch.no_grad():
        features = model.encoder(image_tensor.unsqueeze(0))
        for _ in range(50):
            captions_tensor = torch.LongTensor(result_caption_indices).unsqueeze(0).to(device)
            outputs = model.decoder(features, captions_tensor)
            predicted_index = outputs.argmax(2)[:, -1].item()
            result_caption_indices.append(predicted_index)
            if vocab.itos[predicted_index] == "<EOS>":
                break
    
    result_caption = [vocab.itos[idx] for idx in result_caption_indices]
    return " ".join(result_caption[1:-1])

# --- 4. åˆ›å»ºå¹¶å¯åŠ¨ Gradio åº”ç”¨ ---
demo = gr.Interface(
    fn=predict,
    inputs=gr.Image(type="pil", label="ä¸Šä¼ ä¸€å¼ å›¾ç‰‡"),
    outputs=gr.Textbox(label="AI ç”Ÿæˆçš„å›¾ç‰‡æè¿°"),
    title="ğŸ–¼ï¸ç¬¨ç¬¨ AI çœ‹å›¾è¯´è¯ ğŸ“¸",
    description="ä¸Šä¼ ä»»æ„ä¸€å¼ å›¾ç‰‡ï¼Œè®©AIæ¥å‘Šè¯‰ä½ å®ƒçœ‹åˆ°äº†ä»€ä¹ˆã€‚è¿™ä¸ªæ¨¡å‹ç”±CNNç¼–ç å™¨å’ŒTransformerè§£ç å™¨æ„æˆã€‚",
    examples=None,
    allow_flagging="never"
)

if __name__ == "__main__":
    # æ·»åŠ  share=True ä»¥åœ¨Colabä¸­ç”Ÿæˆå…¬å¼€é“¾æ¥
    demo.launch(share=True)